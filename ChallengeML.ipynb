{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# creazione della classe che ci permette di caricare le immagini e le relative etichette\n",
    "class Dataset(Dataset):\n",
    "    \"\"\"Implementa l'oggetto ClassificationDataset che ci permette di caricare\n",
    "    le immagini del dataset images\"\"\"\n",
    "\n",
    "    def __init__(self, base_path, csv_list, transform=None):\n",
    "        \"\"\"Input:\n",
    "            base_path: il path alla cartella contenente le immagini\n",
    "            txt_list: il path al file di testo contenente la lista delle immagini\n",
    "                        con le relative etichette. Ad esempio train.csv o test.csv.\n",
    "            transform: implementeremo il dataset in modo che esso supporti le trasformazioni\"\"\"\n",
    "        # conserviamo il path alla cartella contenente le immagini\n",
    "        self.base_path = base_path\n",
    "        # carichiamo la lista dei file\n",
    "        # sarà una matrice con n righe (numero di immagini) e 2 colonne (path, etichetta)\n",
    "        #self.images = np.loadtxt(csv_list, dtype=str, delimiter=',', usecols=(0,5))\n",
    "        # sarà una matrice con n righe (numero di immagini) e 6 colonne (path, x, y, u, v, etichetta)\n",
    "        self.images = np.loadtxt(csv_list, dtype=str, delimiter=',')\n",
    "        \n",
    "        # permutiamo i dati\n",
    "        np.random.seed(1234) #impostiamo un seed per avere risultati ripetibili\n",
    "        torch.random.manual_seed(1234);\n",
    "        \n",
    "        idx = np.random.permutation(len(self.images))\n",
    "        self.images = self.images[idx]\n",
    "        #print self.images\n",
    "\n",
    "        # conserviamo il riferimento alla trasformazione da applicare\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # recuperiamo il path dell'immagine di indice index e la relativa etichetta\n",
    "        #f, c = self.images[index]\n",
    "        # recuperiamo il path dell'immagine di indice index, x, y, u, v e la relativa etichetta\n",
    "        f,x,y,u,v,c = self.images[index]\n",
    "\n",
    "        # carichiamo l'immagine utilizzando PIL\n",
    "        im = Image.open(path.join(self.base_path, f))\n",
    "\n",
    "        # se la trasfromazione è definita, applichiamola all'immagine\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "\n",
    "        # convertiamo l'etichetta in un intero\n",
    "        label = int(c)\n",
    "\n",
    "        # restituiamo un dizionario contenente immagine etichetta\n",
    "        #return {'image': im, 'label': label}\n",
    "        # restituiamo un dizionario contenente immagine etichetta posa\n",
    "        return {'image': im, 'label': label, 'pose': np.array([x,y,u,v], dtype='float')}\n",
    "\n",
    "    # restituisce il numero di campioni: la lunghezza della lista \"images\"\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo quindi il nostro training set ed il validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immagine di train: torch.Size([3, 144, 256])\n",
      "Etichetta: 14\n",
      "Posa: [16.397331 -4.223821 -0.999388 -0.034932]\n",
      "\n",
      "Immagine di validation: torch.Size([3, 144, 256])\n",
      "Etichetta: 15\n",
      "Posa: [-19.630225   2.979609   0.038931   0.999242]\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "train = Dataset('dataset/images','dataset/training_list.csv',transform=transforms.ToTensor())\n",
    "sample = train[0]\n",
    "#l'immagine è 3 x 144 x 256 perché è una immagine a colori\n",
    "print \"Immagine di train:\", sample['image'].shape\n",
    "print \"Etichetta:\", sample['label']\n",
    "print \"Posa:\", sample['pose']\n",
    "\n",
    "print \"\"\n",
    "\n",
    "# validation set\n",
    "valid = Dataset('dataset/images','dataset/validation_list.csv',transform=transforms.ToTensor())\n",
    "sample = valid[0]\n",
    "#l'immagine è 3 x 144 x 256 perché è una immagine a colori\n",
    "print \"Immagine di validation:\", sample['image'].shape\n",
    "print \"Etichetta:\", sample['label']\n",
    "print \"Posa:\", sample['pose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuiamo adesso la normalizzazione, e poi per ridurre i tempi computazionali lavoriamo con immagini più piccole, 32x56."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medie [0.38766563 0.36472213 0.35473528]\n",
      "Dev.Std. [0.21211976 0.21056999 0.21186012]\n"
     ]
    }
   ],
   "source": [
    "# per il training set\n",
    "\n",
    "#procedura per calcolare la media\n",
    "m = np.zeros(3)\n",
    "\n",
    "for sample in train:\n",
    "    m+=sample['image'].sum(1).sum(1) #accumuliamo la somma dei pixel canale per canale\n",
    "#dividiamo per il numero di immagini moltiplicato per il numero di pixel\n",
    "m=m/(len(train)*144*256)\n",
    "\n",
    "#procedura simile per calcolare la deviazione standard\n",
    "s = np.zeros(3)\n",
    "\n",
    "for sample in train:\n",
    "    s+=((sample['image']-torch.Tensor(m).view(3,1,1))**2).sum(1).sum(1)\n",
    "s=np.sqrt(s/(len(train)*144*256))\n",
    "\n",
    "# media e devST per i 3 canali\n",
    "print \"Medie\",m\n",
    "print \"Dev.Std.\",s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inseriamo la corretta normalizzazione tra le trasformazioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immagine di train: torch.Size([5376])\n",
      "Etichetta: 14\n",
      "Posa: [16.397331 -4.223821 -0.999388 -0.034932]\n",
      "\n",
      "Immagine di validation: torch.Size([5376])\n",
      "Etichetta: 15\n",
      "Posa: [-19.630225   2.979609   0.038931   0.999242]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(m,s),\n",
    "                               transforms.Lambda(lambda x: x.view(-1))]) #per trasformare l'immagine in un unico vettore\n",
    "train = Dataset('dataset/images','dataset/training_list.csv',transform=transform)\n",
    "print \"Immagine di train:\", train[0]['image'].shape # 3x32x54\n",
    "print \"Etichetta:\", train[0]['label']\n",
    "print \"Posa:\", train[0]['pose']\n",
    "\n",
    "print \"\"\n",
    "\n",
    "valid = Dataset('dataset/images','dataset/validation_list.csv',transform=transform)\n",
    "print \"Immagine di validation:\", valid[0]['image'].shape # 3x32x54\n",
    "print \"Etichetta:\", valid[0]['label']\n",
    "print \"Posa:\", valid[0]['pose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni immagine (sia di train che di validation) quindi è stata normalizzata e trasformata in un vettore. Per effetuare l'ottimizzazione mediante SGD dobbiamo suddividere i campioni in mini-batch. Inoltre è importante fornire i campioni in ordine casuale.\n",
    "PyTorch ci permette di gestire il \"batching\" in automatico e in maniera multithread mediante l'oggetto *DataLoader*. Utilizziamo un batch size di 64 immagini e due thread paralleli per velocizzare il caricamento dei dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=64, num_workers=2, shuffle=True)\n",
    "#shuffle permette di accedere ai dati in maniera casuale\n",
    "valid_loader = DataLoader(valid, batch_size=64, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I data loader sono degli oggetti iterabili. Possiamo dunque accedere ai diversi batch in maniera sequenziale all'interno di un ciclo for. Proviamo ad accedere al primo batch e interrompiamo il ciclo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5376])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "print batch['image'].shape\n",
    "print batch['label'].shape\n",
    "print batch['pose'].shape\n",
    "# il batch contiene 64 vettori di training di dimensione 5376 e altrettante etichette e pose corrispondenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
